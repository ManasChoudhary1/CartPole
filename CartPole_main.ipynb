{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa1f7b61-9ae2-47c3-bea8-6dc7b475a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "from collections import namedtuple,deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0827539d-8eac-4c28-bdd5-e0e298abec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(training_data,mini_batch_size):\n",
    "    experiences = random.sample(training_data, mini_batch_size)\n",
    "    states = tf.convert_to_tensor(\n",
    "        np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    dones = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return (states, actions,  next_states,rewards, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "246252a1-25c8-4139-a59e-6dc47afb7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_move(q_values,epsilon):\n",
    "        if random.random()>epsilon:\n",
    "            return np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            return random.choice(np.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4875145-dc56-40f8-b2c2-3b36f749b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_loss(q_network, tq_network, mini_batch, gamma):\n",
    "    states, actions, next_states, rewards, dones = mini_batch\n",
    "\n",
    "    # Compute target Q-values\n",
    "    next_q_values = tq_network(next_states)  # shape: (batch_size, num_actions)\n",
    "    max_next_q = tf.reduce_max(next_q_values, axis=1)  # shape: (batch_size,)\n",
    "    targets = rewards + (1.0 - dones) * gamma * max_next_q  # shape: (batch_size,)\n",
    "\n",
    "    # Current Q-values\n",
    "    q_values = q_network(states)  # shape: (batch_size, num_actions)\n",
    "    batch_indices = tf.range(tf.shape(q_values)[0])  # shape: (batch_size,)\n",
    "    action_indices = tf.cast(actions, tf.int32)\n",
    "    q_taken = tf.gather_nd(q_values, tf.stack([batch_indices, action_indices], axis=1))  # shape: (batch_size,)\n",
    "\n",
    "    return tf.reduce_mean(tf.square(q_taken - targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd13b717-917a-4f72-88b8-cd449005ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to our update target network\n",
    "def update_networks(q_network,tq_network,tau):\n",
    "    q_weights = q_network.get_weights()\n",
    "    tq_weights = tq_network.get_weights()\n",
    "    updated_weights = [\n",
    "        (1-tau )* tq_w + (tau) * q_w\n",
    "        for tq_w, q_w in zip(tq_weights, q_weights)\n",
    "    ]\n",
    "    tq_network.set_weights(updated_weights)\n",
    "    return tq_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6845f61-054b-4e82-acc9-769823784466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(q_network,tq_network,gamma,update_interval, mini_batch_size,training_data,iterations = 10000,e=1,e_min=0.01,e_decay=0.995,tau = 1e-3):\n",
    "    scores = []\n",
    "    for iteration in range(1,iterations+1):\n",
    "        \n",
    "        # Reset the environment to the initial state and get the initial state\n",
    "        state,_ = env.reset()\n",
    "        score = 0\n",
    "        for i in range(300):\n",
    "            state_qn = np.expand_dims(state,axis = 0) \n",
    "            q_values = q_network(state_qn)\n",
    "            action = play_move(q_values,e)\n",
    "            next_state,reward,terminated,truncated,_ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            training_data.append(experiences(state,action,next_state,reward,done))\n",
    "            score+= reward;\n",
    "            if i%update_interval ==0:\n",
    "                mini_batch = get_experiences(training_data,mini_batch_size)\n",
    "                states, actions, next_states,rewards,dones = mini_batch\n",
    "              \n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = q_loss(q_network,tq_network, mini_batch,gamma)\n",
    "                grads = tape.gradient(loss,q_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "                update_networks(q_network,tq_network,tau)\n",
    "                \n",
    "            if done:\n",
    "                break;\n",
    "            state = next_state\n",
    "        scores.append(score)\n",
    "        if iteration%100 == 0 :\n",
    "            \n",
    "            avg_score =  np.mean(scores[-100:])\n",
    "            print(f\"average score after {iteration} iterations is\",avg_score )\n",
    "            if avg_score > 195:\n",
    "                \"cartpole problem solved :)\" \n",
    "                break\n",
    "        e = max(e*e_decay,e_min)\n",
    "        \n",
    "    return q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ecca769-d1e3-4567-9d29-ba09b0fdc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing our gym environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6695d99e-4ac3-4a3f-ac03-2b7008367d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting some random data\n",
    "experiences = namedtuple(\"experiences\",['state','action','next_state','reward','done'])\n",
    "training_data = deque(maxlen=10000)\n",
    "\n",
    "episodes = 256\n",
    "for episode in range(1, episodes +1):\n",
    "    state,_ = env.reset()\n",
    "    done  = False\n",
    "    while not done:\n",
    "        action = random.choice([0,1])\n",
    "        next_state,reward,terminated,truncated,_ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "    training_data.append(experiences(state,action,next_state,reward,done))\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55ea3115-6fc8-483b-82e3-1a760da900ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_q=((training_data[0][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f80f20a1-d5fc-4bb6-82b0-5bb37bc747f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# intializing our neural network layers\n",
    "q_network = Sequential([\n",
    "    Input(shape = input_shape_q),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2,activation = 'linear')\n",
    "])\n",
    "tq_network = Sequential([\n",
    "    Input(shape = input_shape_q),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2,activation = 'linear')\n",
    "])\n",
    "final_q_network = Sequential([\n",
    "    Input(shape = input_shape_q),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2,activation = 'linear')\n",
    "])\n",
    "optimizer = Adam(learning_rate = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4638803e-70aa-473a-aa3d-683b3a0bcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting same weights in both\n",
    "tq_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272171e-c8dd-4bab-97e4-b28b0dd95c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "gamma = 0.98\n",
    "mini_batch_size = 64\n",
    "update_interval = 5\n",
    "num_p_av = 100  \n",
    "tau = 1e-3\n",
    "e = 1 #epsilon\n",
    "e_min = 0.1 \n",
    "e_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbec247-359a-45f3-981c-53b9237f670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score after 100 iterations is 18.68\n",
      "average score after 200 iterations is 13.62\n",
      "average score after 300 iterations is 11.23\n",
      "average score after 400 iterations is 10.38\n",
      "average score after 500 iterations is 10.2\n",
      "average score after 600 iterations is 10.51\n",
      "average score after 700 iterations is 11.3\n",
      "average score after 800 iterations is 14.24\n",
      "average score after 900 iterations is 58.71\n",
      "average score after 1000 iterations is 128.03\n",
      "average score after 1100 iterations is 189.23\n",
      "average score after 1200 iterations is 288.29\n"
     ]
    }
   ],
   "source": [
    "final_q_network.set_weights(train(q_network,tq_network,gamma,update_interval, mini_batch_size,training_data,iterations = 10000,e=1,e_min=0.01,e_decay=0.995,tau = 1e-3).get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8366d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_q_network.save(\"Q_network_cartpole.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25027e5b-104d-48af-8674-d5cdebf7b9c6",
   "metadata": {},
   "source": [
    "# Final Words\n",
    "\n",
    "Finally, we have our **final_q_network**, trains for 1200 iterations. It achieves an average score of **288**, which is well above the threshold of 195 required to consider the CartPole problem solved. Although this is a simple and foundational reinforcement learning problem, it still effectively demonstrates the application of core concepts.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We defined the following essential functions:\n",
    "\n",
    "1. **get_experiences** – Returns a minibatch of experience samples to train the network.  \n",
    "2. **play_move** – Returns the optimal action based on the Q-network’s predicted Q-values.  \n",
    "3. **q_loss** – Calculates the loss between the predicted Q-values from the Q-network and the target Q-network.  \n",
    "4. **update_networks** – Updates the target network parameters periodically, every `update_interval` steps.  \n",
    "5. **train** – Runs the training loop for a specified number of iterations (2000 in our case). At each iteration, it chooses an action based on the current Q-network, stores the resulting experience in the training data, and updates the Q-network periodically to move closer to the target Q-network.\n",
    "\n",
    "## Working and Observations\n",
    "\n",
    "We first set up the environment and collected 256 initial training examples to bootstrap the training process. Then we started training the model.\n",
    "\n",
    "After experimenting with hyperparameters to speed up convergence, I noticed that the network took nearly 1000 iterations before meaningful learning began due to the initial 256 examples collected via random actions. Since these random examples generally yield low rewards and short episodes, they don’t dramatically impact total training time but do affect early learning speed.\n",
    "\n",
    "The hyperparameters used are detailed separately. Feel free to modify them to see how they affect the model’s convergence and overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d799f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
